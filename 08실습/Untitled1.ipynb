{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2  하이퍼파라미터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "word_vec_size = 256\n",
    "dropout_p = 0.3\n",
    "\n",
    "hidden_size = 512\n",
    "num_layers = 4\n",
    "\n",
    "## yhk 추가\n",
    "learning_rate = 0.001 # 디폴트 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SMS train, test dataset 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = DataLoader(\n",
    "    train_fn = './sms.maxlen.uniq.shuf.train.tsv',\n",
    "    batch_size = batch_size,\n",
    "    valid_ratio = .2,\n",
    "    device = -1,\n",
    "    max_vocab = 999999,\n",
    "    min_freq = 5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    train_fn = './sms.maxlen.uniq.shuf.train.tsv',\n",
    "    batch_size = batch_size,\n",
    "    valid_ratio = .01, #  모두 train\n",
    "    device = -1,\n",
    "    max_vocab = 999999,\n",
    "    min_freq = 5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 대략적인 데이터 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('|train| =', 3722, '|valid| =', 930)\n",
      "('|vocab| =', 1576, '|classes| =', 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"|train| =\", len(loaders.train_loader.dataset),\n",
    "     '|valid| =', len(loaders.valid_loader.dataset))\n",
    "\n",
    "vocab_size = len(loaders.text.vocab)\n",
    "num_classes = len(loaders.label.vocab)\n",
    "print('|vocab| =', vocab_size, '|classes| =', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 데이터 로드함수\n",
    "   학습시킬 때 batch_size단위로 끊어서 로드하기 위함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로드함수 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "('\\xed\\x95\\x9c \\xeb\\xb2\\x88\\xec\\x97\\x90 \\xeb\\xa1\\x9c\\xeb\\x93\\x9c\\xeb\\x90\\x98\\xeb\\x8a\\x94 \\xeb\\x8d\\xb0\\xec\\x9d\\xb4\\xed\\x84\\xb0 \\xed\\x81\\xac\\xea\\xb8\\xb0:', 128)\n",
      "('label: ', array(0))\n",
      "('text: ', (6,))\n",
      "('label: ', array(0))\n",
      "('text: ', (6,))\n",
      "('label: ', array(0))\n",
      "('text: ', (6,))\n",
      "[1]\n",
      "('\\xed\\x95\\x9c \\xeb\\xb2\\x88\\xec\\x97\\x90 \\xeb\\xa1\\x9c\\xeb\\x93\\x9c\\xeb\\x90\\x98\\xeb\\x8a\\x94 \\xeb\\x8d\\xb0\\xec\\x9d\\xb4\\xed\\x84\\xb0 \\xed\\x81\\xac\\xea\\xb8\\xb0:', 128)\n",
      "('label: ', array(0))\n",
      "('text: ', (9,))\n",
      "('label: ', array(0))\n",
      "('text: ', (9,))\n",
      "('label: ', array(0))\n",
      "('text: ', (9,))\n",
      "[2]\n",
      "('\\xed\\x95\\x9c \\xeb\\xb2\\x88\\xec\\x97\\x90 \\xeb\\xa1\\x9c\\xeb\\x93\\x9c\\xeb\\x90\\x98\\xeb\\x8a\\x94 \\xeb\\x8d\\xb0\\xec\\x9d\\xb4\\xed\\x84\\xb0 \\xed\\x81\\xac\\xea\\xb8\\xb0:', 128)\n",
      "('label: ', array(0))\n",
      "('text: ', (21,))\n",
      "('label: ', array(0))\n",
      "('text: ', (21,))\n",
      "('label: ', array(0))\n",
      "('text: ', (21,))\n",
      "[3]\n",
      "('\\xed\\x95\\x9c \\xeb\\xb2\\x88\\xec\\x97\\x90 \\xeb\\xa1\\x9c\\xeb\\x93\\x9c\\xeb\\x90\\x98\\xeb\\x8a\\x94 \\xeb\\x8d\\xb0\\xec\\x9d\\xb4\\xed\\x84\\xb0 \\xed\\x81\\xac\\xea\\xb8\\xb0:', 128)\n",
      "('label: ', array(0))\n",
      "('text: ', (12,))\n",
      "('label: ', array(0))\n",
      "('text: ', (12,))\n",
      "('label: ', array(0))\n",
      "('text: ', (12,))\n"
     ]
    }
   ],
   "source": [
    "n = 3 # 샘플로 출력할 데이터 개수\n",
    "for i, data in enumerate(loaders.train_loader):\n",
    "    labels = data.label\n",
    "    texts = data.text\n",
    "    \n",
    "    if i > n:\n",
    "        break\n",
    "    print( \"[%d]\" %i)\n",
    "    print(\"한 번에 로드되는 데이터 크기:\", len(labels))\n",
    "    \n",
    "    # 출력\n",
    "    for j in range(n):\n",
    "        label = labels[j].numpy() # tensor -> numpy 로 변환\n",
    "        text = texts[j].numpy()\n",
    "        print( \"label: \", label)\n",
    "        print(\"text: \", text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recurrent neural network (many to one)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_size, \n",
    "                word_vec_size, \n",
    "                hidden_size, \n",
    "                n_classes, \n",
    "                num_layers = 4, \n",
    "                dropout_p = 0.3\n",
    "               ):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.word_vec_size = word_vec_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_classes = n_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        self.emb = nn.Embedding(input_size, word_vec_size)\n",
    "        self.lstm = nn.LSTM(input_size = word_vec_size,\n",
    "                           hidden_size = hidden_size,\n",
    "                            num_layers = num_layers,\n",
    "                            dropout=dropout_p,\n",
    "                            batch_first = True,\n",
    "                            bidirectional = True)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "        \n",
    "        self.activation = nn.LogSoftmax(dim=-1)\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        \n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        out = self.activation(self.fc(x[:,-1]))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size = vocab_size, \n",
    "            word_vec_size = word_vec_size, \n",
    "            hidden_size = hidden_size, \n",
    "            n_classes = num_classes, \n",
    "            num_layers = num_layers, \n",
    "            dropout_p = dropout_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeAccr(dloader, imodel):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    model.eval()\n",
    "    for i, data in enumerate(dloader):\n",
    "        texts = data.text.to(device)\n",
    "        labels = data.label.to(device)\n",
    "        \n",
    "        output = model(texts)\n",
    "        _, output_index = torch.max(output, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (output_index == labels).sum().float()\n",
    "        \n",
    "    model.train()\n",
    "    return (100*correct/total).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Data: 89.03\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Test Data: %.2f\" %ComputeAccr(loaders.valid_loader, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [1/10], Step [10/30], Loss: 0.4073, Accr: 89.03\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [1/10], Step [20/30], Loss: 0.0867, Accr: 89.03\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [1/10], Step [30/30], Loss: 0.2213, Accr: 89.03\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [2/10], Step [10/30], Loss: 0.4348, Accr: 78.39\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [2/10], Step [20/30], Loss: 2.2580, Accr: 93.23\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [2/10], Step [30/30], Loss: 0.2258, Accr: 89.03\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [3/10], Step [10/30], Loss: 0.1523, Accr: 89.35\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [3/10], Step [20/30], Loss: 0.1738, Accr: 92.80\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [3/10], Step [30/30], Loss: 0.0515, Accr: 95.05\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [4/10], Step [10/30], Loss: 0.4765, Accr: 93.98\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [4/10], Step [20/30], Loss: 0.0381, Accr: 93.66\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [4/10], Step [30/30], Loss: 0.0617, Accr: 94.30\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [5/10], Step [10/30], Loss: 0.5597, Accr: 95.27\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [5/10], Step [20/30], Loss: 0.0732, Accr: 95.38\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [5/10], Step [30/30], Loss: 0.0373, Accr: 94.19\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [6/10], Step [10/30], Loss: 0.2863, Accr: 94.41\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [6/10], Step [20/30], Loss: 0.0117, Accr: 93.98\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [6/10], Step [30/30], Loss: 0.0518, Accr: 95.59\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [7/10], Step [10/30], Loss: 0.0549, Accr: 95.91\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [7/10], Step [20/30], Loss: 0.0559, Accr: 95.91\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [7/10], Step [30/30], Loss: 0.0167, Accr: 95.91\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [8/10], Step [10/30], Loss: 0.0129, Accr: 96.13\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [8/10], Step [20/30], Loss: 0.0842, Accr: 97.20\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [8/10], Step [30/30], Loss: 0.0131, Accr: 95.48\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [9/10], Step [10/30], Loss: 0.0124, Accr: 94.84\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [9/10], Step [20/30], Loss: 0.0472, Accr: 95.38\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [9/10], Step [30/30], Loss: 0.0092, Accr: 96.13\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "Epoch [10/10], Step [10/30], Loss: 0.0512, Accr: 97.31\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "Epoch [10/10], Step [20/30], Loss: 0.0247, Accr: 96.99\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "Epoch [10/10], Step [30/30], Loss: 0.0079, Accr: 97.10\n"
     ]
    }
   ],
   "source": [
    "total_step = len(loaders.train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(loader.train_loader):\n",
    "        texts = data.text.to(device)\n",
    "        labels = data.label.to(device)\n",
    "        \n",
    "        print(\"[%d]\" %i)\n",
    "        \n",
    "        outputs = model(texts)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if(i + 1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accr: {:.2f}'\n",
    "                 .format(epoch+1, num_epochs, i+1, total_step, loss.item(), ComputeAccr(loaders.valid_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ouf Valid Data: 97.10\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy ouf Valid Data: %.2f\" %ComputeAccr(loaders.valid_loader, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p27/lib/python2.7/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "netname = './rnn_weight.pkl'\n",
    "torch.save(model, netname,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p27",
   "language": "python",
   "name": "conda_pytorch_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
